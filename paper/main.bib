@inproceedings{choromanska2015,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  year={2015}
}

@inproceedings{kawaguchi2016,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{safran2018,
  title={Spurious local minima are common in two-layer ReLU neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@article{venturi2019,
  title={Spurious valleys in one-hidden-layer neural network optimization landscapes},
  author={Venturi, Luca and Bandeira, Afonso S and Bruna, Joan},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={133},
  pages={1--34},
  year={2019}
}

@inproceedings{du2019,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{allenzhu2019,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@article{zou2020,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  pages={467--492},
  year={2020}
}

@inproceedings{jacot2018,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@inproceedings{pennington2017,
  title={Nonlinear random matrix theory for deep learning},
  author={Pennington, Jeffrey and Worah, Pratik},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{louart2018,
  title={A random matrix approach to neural networks},
  author={Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
  journal={The Annals of Applied Probability},
  volume={28},
  number={2},
  pages={1190--1248},
  year={2018}
}

@book{anderson2010,
  title={An introduction to random matrices},
  author={Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
  year={2010},
  publisher={Cambridge university press}
}

@article{tao2012,
  title={Random covariance matrices: Universality of local statistics of eigenvalues},
  author={Tao, Terence and Vu, Van},
  journal={The Annals of Probability},
  volume={40},
  number={3},
  pages={1285--1315},
  year={2012}
}

@article{erdos2012,
  title={Rigidity of eigenvalues of generalized Wigner matrices},
  author={Erd{\H{o}}s, L{\'a}szl{\'o} and Yau, Horng-Tzer and Yin, Jun},
  journal={Advances in Mathematics},
  volume={229},
  number={3},
  pages={1435--1515},
  year={2012}
}

@inproceedings{gunasekar2018,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  year={2018}
}

@inproceedings{lyu2020,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{mei2018,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018}
}

@article{geiger2019,
  title={Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
  author={Geiger, Mario and Spigler, Stefano and d'Ascoli, St{\'e}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
  journal={Physical Review E},
  volume={100},
  number={1},
  pages={012115},
  year={2019}
}

@inproceedings{sagun2017,
  title={Empirical analysis of the Hessian of over-parametrized neural networks},
  author={Sagun, Levent and Evci, Utku and G{\"u}ney, V U{\u{g}}ur and Dauphin, Yann and Bottou, L{\'e}on},
  booktitle={International Conference on Learning Representations Workshop},
  year={2018}
}

@article{ghorbani2019,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  pages={1029--1054},
  year={2021}
}

@article{benarous2019,
  title={Online stochastic gradient descent on non-convex losses from high-dimensional inference},
  author={Ben Arous, G{\'e}rard and Gheissari, Reza},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={106},
  pages={1--51},
  year={2021}
}

@article{auffinger2013,
  title={Random matrices and complexity of spin glasses},
  author={Auffinger, Antonio and Ben Arous, Gerard and {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={165--201},
  year={2013}
}

@article{knowlesyin2017,
  title={Anisotropic local laws for random matrices},
  author={Knowles, Antti and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={169},
  number={1},
  pages={257--352},
  year={2017}
}
